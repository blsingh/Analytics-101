---
title: "Trees, classification & regression"
author: "Balsher Singh"
date: "July 7, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE, suppressMessages = TRUE, suppressWarnings = TRUE)
```

```{r, include= FALSE}
setwd("c:/study/Analy_Edge/04_CART")
# Load the gerber.csv to get started.
gerber <- read.csv("gerber.csv")
```


1.1 Expoloration and Logistics Regression.  What percentage of the people voted in the election?

```{r}
table(gerber$voting)['1']/sum(table(gerber$voting))
```

1.2 Which of the 4 treatment groups(hawthorne, civicduty, neighbors, and self) had the largest percentage of people who actually voted?

```{r}
names(gerber)

sapply(gerber[c('hawthorne', 'civicduty','neighbors','self')], function(x) sum(x==1 & gerber$voting==1))/sapply(gerber[c('hawthorne', 'civicduty', 'neighbors','self')],sum)
# Or use tapply/by to split voting by each column 
sapply(gerber[c('hawthorne', 'civicduty','neighbors','self')], function(x) by(gerber$voting,x,mean)) # IS THIS EFFICIENT

```

1.3 Build a logistic regression model for voting using the four treatment group variables as the independent variables (civicduty, hawthorne, self, and neighbors). Use all the data to build the model (DO NOT split the data into a training set and testing set). Which of the following coefficients are significant in the logistic regression model?

```{r}
Logmodel <- glm(voting ~ hawthorne + civicduty + neighbors + self, data = gerber, family = 'binomial')

summary(Logmodel)
```

1.4 Using a threshold of 0.3, what is the accuracy of the logistic regression model?
```{r}
Logmodel.pred <- predict(Logmodel, type = "response")

table(gerber$voting, Logmodel.pred > .3)

sum(diag(table(gerber$voting, Logmodel.pred > .3)))/sum(table(gerber$voting, Logmodel.pred > .3))
```
1.5 Using a threshold of 0.5, what is the accuracy of the logistic regression model?
```{r}
sum(diag(table(gerber$voting, Logmodel.pred > .5)))/sum(table(gerber$voting, Logmodel.pred > .5))

```


1.6 [BASELINE MODEL] Compare your previous two answers to the percentage of people who did not vote (the baseline accuracy) and compute the AUC of the model. What is happening here?

```{r}
table(gerber$voting)['0']/sum(table(gerber$voting))

# What is ... the percentage of people how did not vote in the baseline model, is the same as the accuracy of the logistic model above at threshold of 0.5.

# Compute the AUC

library(ROCR)
RocrPred <- prediction(Logmodel.pred, gerber$voting)
auc <- as.numeric(performance(RocrPred,"auc")@y.values)
auc
```



### Problem 2 Trees
 2.1 We will now try out trees. Build a CART tree for voting using all data and the same four treatment variables we used before. Don't set the option method="class" - we are actually going to create a regression tree here. We are interested in building a tree to explore the fraction of people who vote, or the probability of voting. We'd like CART to split our groups if they have different probabilities of voting. If we used method='class', CART would only split if one of the groups had a probability of voting above 50% and the other had a probability of voting less than 50% (since the predicted outcomes would be different). However, with regression trees, CART will split even if both groups have probability less than 50%.
 
 
 
 
```{r}
library(rpart)
library(rpart.plot)
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)

prp(CARTmodel)
```
2.2
```{r}
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel2)
    ```

2.3 Using only the CART tree plot, determine what fraction (a number between 0 and 1) of "Civic Duty" people voted:
```{r}
mod2.pred <- predict(CARTmodel2, newdata = gerber[gerber$civicduty == 1, ])

```

2.4 Make a new tree that includes the "sex" variable, again with cp = 0.0. Notice that sex appears as a split that is of secondary importance to the treatment group.
```{r}
CARTmodel3 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0)

CARTmodel4 = rpart(voting ~ civicduty + hawthorne + self + neighbors + control + sex, data=gerber, cp=0.0)

prp(CARTmodel3)
prp(CARTmodel4)
```


3.3 VGoing back to logistic regression now, create a model using "sex" and "control". Interpret the coefficient for "sex":

```{r}
mod3.3 <- glm(voting ~ control + sex, data = gerber, family = 'binomial')
summary(mod3.3)

```

3.4 The regression tree calculated the percentage voting exactly for every one of the four possibilities (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control). Logistic regression has attempted to do the same, although it wasn't able to do as well because it can't consider exactly the joint possibility of being a women and in the control group.

We can quantify this precisely. Create the following dataframe (this contains all of the possible values of sex and control), and evaluate your logistic regression using the predict function (where "LogModelSex" is the name of your logistic regression model that uses both control and sex):

```{r}
Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
setNames(predict(mod3.3, newdata=Possibilities, type="response"),c("Man/not ctrl", "Man/ctrl", "Woman/not ctrl", "Woman/ctrl"))

```

The four values in the results correspond to the four possibilities in the order they are stated above ( (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control) ). What is the absolute difference between the tree and the logistic regression for the (Woman, Control) case? Give an answer with five numbers after the decimal point.
 
 
```{r}
# In 3.1 -3.3 we made a TREE regression model:
CARTmodel0 <- rpart(voting ~ control + sex , data = gerber, cp =0.0)
prp(CARTmodel0, digits = 6)

# A way to extract these values was use the predict:
setNames(predict(CARTmodel0, newdata = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))), c("man/not ctrl", "man/ctrl", "woman/not ctrl", "woman ctrl"))

# Thus the abs diff between two model prediction for the Woman/ctrl case is :
abs(0.2904558 - 0.2908065)
```

3.5 Interaction term
How do you interpret the coefficient for the new variable in isolation? That is, how does it relate to the dependent variable?
```{r}
LogModel2 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")

summary(LogModel2)

```

3.6 Now what is the difference between the logistic regression model and the CART model for the (Woman, Control) case? Again, give your answer with five numbers after the decimal point.

```{r}
abs( predict(LogModel2, newdata = data.frame(sex=1, control = 1), type = "response") -
predict(CARtmodel00 , newdata =data.frame(sex=1, control=1)) )
```

