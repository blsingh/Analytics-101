ls()
## Downloaded the data in finals folder
#Load park_visits.csv into a data frame called visits.
visits = read.csv("park_visits.csv", stringsAsFactors=F)
#1
#Let's first look at the visits in July 2016. Subset the observations to this year and month, name it visits2016jul. Work with this data subset for the next three problems.
visits0716 <- subset(visits, Year==2016 & Month==7)
#Which park type has the most number of parks?
table(visits0716$ParkType)
#Which specific park has the most number of visitors?
## Group rows by 'ParkName' and sum the 'log Visits'
visits0716 %>% group_by(ParkName) %>% summarise(sum=sum(logVisits, na.rm=T)) %>% arrange(desc(sum))
#Which region has the highest average log visits in July 2016?
sort(head(with(visits0716, tapply(logVisits, Region, function(x) sum(x)/length(x)))))
#
cor(visits0716$cost, visits0716$logVisits)
#Let's now look at the time dimension of the data. Subset the original data (visits) to "Yellowstone NP" only and save as ys. Use the following code to plot the logVisits through the months between 2010 and 2016:
ys <- subset(visits, ParkName == "Yellowstone NP")
ys_ts=ts(ys$logVisits,start=c(2010,1),freq=12)
plot(ys_ts)
#To deal with the missing values, we will simply remove the observations with the missing values first (there are more sophisticated ways to work with missing values, but for this purpose removing the observations is fine). Run the following:
visits = visits[rowSums(is.na(visits)) == 0, ]
#
visits$Month = as.factor(visits$Month)
#
train <- subset(visits, Year <=2014)
test <- subset(visits, Year > 2014)
#Let's build now a simple linear regression model "mod" using the training set to predict the log visits. As a first step, we only use the laglogVisits variable (log visits from last month).
mod = lm(logVisits ~ laglogVisits, data = train)
summary(mod)
#* What's the out-of-sample R2 in the testing set for this simple model?
pred = predict(mod, newdata = test)
SSE = sum((test$logVisits - pred)^2)
SST = sum((test$logVisits - mean(test$logVisits))^2)
1-SSE/SST
# We see that the model achieves good predictive power already simply using the previous month's visits. To see if the other knowledge we have about the parks can improve the model, let's add these variables in a new model.
#The new model would have the following variables:
#
#laglogVisits, laglogVisitsYear, Year, Month, Region, ParkType, and cost
mod2 = lm(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train)
summary(mod2)
#In the new model, what's the out-of-sample R2 in the testing set?
pred2 = predict(mod2, newdata = test)
SSE2 = sum((test$logVisits-pred2)^2)
SST # remins the same i.e. base model remains the same.
1-SSE2/SST
# Regression Trees
library(rpart)
#In addition to the logistic regression model, we can also train a regression tree. Use the same set of variables as the previous problem (laglogVisits, laglogVisitsYear, Year, Month, Region, ParkType, and cost), train a regression tree with cp = 0.05.
caRt1 <- rpart(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train, method = "anova", cp = 0.05)
library(rpart.plot)
prp(caRt1) # How many different predicted values are there? 
summary(caRt1) 
#What is the out-of-sample R2 on the testing set?
predcaRt = predict(caRt1, newdata = test)
SSEcaRt = sum((test$logVisits - predcaRt)^2)
SST # remains the same
1-SSEcaRt/SST
library(colorout)
library(colorout)
ls()
list.files()
setwd("/mnt/c/Study/Analy_Edge/10_FINAL")
list.files()
## Downloaded the data in finals folder
#Load park_visits.csv into a data frame called visits.
visits = read.csv("park_visits.csv", stringsAsFactors=F)
#1
#Let's first look at the visits in July 2016. Subset the observations to this year and month, name it visits2016jul. Work with this data subset for the next three problems.
visits0716 <- subset(visits, Year==2016 & Month==7)
#Which park type has the most number of parks?
table(visits0716$ParkType)
#Which specific park has the most number of visitors?
## Group rows by 'ParkName' and sum the 'log Visits'
visits0716 %>% group_by(ParkName) %>% summarise(sum=sum(logVisits, na.rm=T)) %>% arrange(desc(sum))
#Which region has the highest average log visits in July 2016?
sort(head(with(visits0716, tapply(logVisits, Region, function(x) sum(x)/length(x)))))
#
cor(visits0716$cost, visits0716$logVisits)
#Let's now look at the time dimension of the data. Subset the original data (visits) to "Yellowstone NP" only and save as ys. Use the following code to plot the logVisits through the months between 2010 and 2016:
ys <- subset(visits, ParkName == "Yellowstone NP")
ys_ts=ts(ys$logVisits,start=c(2010,1),freq=12)
plot(ys_ts)
#To deal with the missing values, we will simply remove the observations with the missing values first (there are more sophisticated ways to work with missing values, but for this purpose removing the observations is fine). Run the following:
visits = visits[rowSums(is.na(visits)) == 0, ]
#
visits$Month = as.factor(visits$Month)
#
train <- subset(visits, Year <=2014)
test <- subset(visits, Year > 2014)
#Let's build now a simple linear regression model "mod" using the training set to predict the log visits. As a first step, we only use the laglogVisits variable (log visits from last month).
mod = lm(logVisits ~ laglogVisits, data = train)
summary(mod)
#* What's the out-of-sample R2 in the testing set for this simple model?
pred = predict(mod, newdata = test)
SSE = sum((test$logVisits - pred)^2)
SST = sum((test$logVisits - mean(test$logVisits))^2)
1-SSE/SST
# We see that the model achieves good predictive power already simply using the previous month's visits. To see if the other knowledge we have about the parks can improve the model, let's add these variables in a new model.
#The new model would have the following variables:
#
#laglogVisits, laglogVisitsYear, Year, Month, Region, ParkType, and cost
mod2 = lm(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train)
summary(mod2)
#In the new model, what's the out-of-sample R2 in the testing set?
pred2 = predict(mod2, newdata = test)
SSE2 = sum((test$logVisits-pred2)^2)
SST # remins the same i.e. base model remains the same.
1-SSE2/SST
# Regression Trees
library(rpart)
#In addition to the logistic regression model, we can also train a regression tree. Use the same set of variables as the previous problem (laglogVisits, laglogVisitsYear, Year, Month, Region, ParkType, and cost), train a regression tree with cp = 0.05.
caRt1 <- rpart(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train, method = "anova", cp = 0.05)
library(rpart.plot)
prp(caRt1) # How many different predicted values are there? 
summary(caRt1) 
#What is the out-of-sample R2 on the testing set?
predcaRt = predict(caRt1, newdata = test)
SSEcaRt = sum((test$logVisits - predcaRt)^2)
SST # remains the same
1-SSEcaRt/SST
1-SSEcaRt/SST
q()
