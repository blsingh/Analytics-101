library(colorout)
ls()
plot(199)
system2("vim ~/.tmux.conf")
list.file
## Downloaded the data in finals folder
#Load park_visits.csv into a data frame called visits.
visits = read.csv("park_visits.csv", stringsAsFactors=F)
setwd("/mnt/c/Study/Analy_a0o
#1
#Let's first look at the visits in July 2016. Subset the observations to this year and month, name it visits2016jul. Work with this data subset for the next three problems.
visits0716 <- subset(visits, Year==2016 & Month==7)
#Which park type has the most number of parks?
table(visits0716$ParkType)
#Which specific park has the most number of visitors?
## Group rows by 'ParkName' and sum the 'log Visits'
visits0716 %>% group_by(ParkName) %>% summarise(sum=sum(logVisits, na.rm=T)) %>% arrange(desc(sum))
#Which region has the highest average log visits in July 2016?
sort(head(with(visits0716, tapply(logVisits, Region, function(x) sum(x)/length(x)))))
#
cor(visits0716$cost, visits0716$logVisits)
#Let's now look at the time dimension of the data. Subset the original data (visits) to "Yellowstone NP" only and save as ys. Use the following code to plot the logVisits through the months between 2010 and 2016:
ys <- subset(visits, ParkName == "Yellowstone NP")
ys_ts=ts(ys$logVisits,start=c(2010,1),freq=12)
plot(ys_ts)
#To deal with the missing values, we will simply remove the observations with the missing values first (there are more sophisticated ways to work with missing values, but for this purpose removing the observations is fine). Run the following:
visits = visits[rowSums(is.na(visits)) == 0, ]
#
visits$Month = as.factor(visits$Month)
#
train <- subset(visits, Year <=2014)
test <- subset(visits, Year > 2014)
#Let's build now a simple linear regression model "mod" using the training set to predict the log visits. As a first step, we only use the laglogVisits variable (log visits from last month).
mod = lm(logVisits ~ laglogVisits, data = train)
summary(mod)
#* What's the out-of-sample R2 in the testing set for this simple model?
pred = predict(mod, newdata = test)
SSE = sum((test$logVisits - pred)^2)
SST = sum((test$logVisits - mean(test$logVisits))^2)
1-SSE/SST
# We see that the model achieves good predictive power already simply using the previous month's visits. To see if the other knowledge we have about the parks can improve the model, let's add these variables in a new model.
#The new model would have the following variables:
#
#laglogVisits, laglogVisitsYear, Year, Month, Region, ParkType, and cost
mod2 = lm(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train)
summary(mod2)
#In the new model, what's the out-of-sample R2 in the testing set?
pred2 = predict(mod2, newdata = test)
SSE2 = sum((test$logVisits-pred2)^2)
SST # remins the same i.e. base model remains the same.
1-SSE2/SST
# Regression Trees
library(rpart)
#In addition to the logistic regression model, we can also train a regression tree. Use the same set of variables as the previous problem (laglogVisits, laglogVisitsYear, Year, Month, Region, ParkType, and cost), train a regression tree with cp = 0.05.
caRt1 <- rpart(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train, method = "anova", cp = 0.05)
library(rpart.plot)
prp(caRt1) # How many different predicted values are there? 
summary(caRt1) 
#What is the out-of-sample R2 on the testing set?
predcaRt = predict(caRt1, newdata = test)
SSEcaRt = sum((test$logVisits - predcaRt)^2)
SST # remains the same
1-SSEcaRt/SST
#Regression Trees with CV
#▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
# refer to Recitation from analytic edge for further help
setwd("/mnt/c/Study/Analy_Edge/10_FINAL/")
list.files()
library(dplyr)
## Downloaded the data in finals folder
#Load park_visits.csv into a data frame called visits.
visits = read.csv("park_visits.csv", stringsAsFactors=F)
setwd("mnt/c/Study/Analy_Edge/10_FINAL")
#1
#Let's first look at the visits in July 2016. Subset the observations to this year and month, name it visits2016jul. Work with this data subset for the next three problems.
visits0716 <- subset(visits, Year==2016 & Month==7)
#Which park type has the most number of parks?
table(visits0716$ParkType)
#Which specific park has the most number of visitors?
## Group rows by 'ParkName' and sum the 'log Visits'
visits0716 %>% group_by(ParkName) %>% summarise(sum=sum(logVisits, na.rm=T)) %>% arrange(desc(sum))
#Which region has the highest average log visits in July 2016?
sort(head(with(visits0716, tapply(logVisits, Region, function(x) sum(x)/length(x)))))
#
cor(visits0716$cost, visits0716$logVisits)
#Let's now look at the time dimension of the data. Subset the original data (visits) to "Yellowstone NP" only and save as ys. Use the following code to plot the logVisits through the months between 2010 and 2016:
ys <- subset(visits, ParkName == "Yellowstone NP")
ys_ts=ts(ys$logVisits,start=c(2010,1),freq=12)
plot(ys_ts)
#To deal with the missing values, we will simply remove the observations with the missing values first (there are more sophisticated ways to work with missing values, but for this purpose removing the observations is fine). Run the following:
visits = visits[rowSums(is.na(visits)) == 0, ]
#
visits$Month = as.factor(visits$Month)
#
train <- subset(visits, Year <=2014)
test <- subset(visits, Year > 2014)
#Let's build now a simple linear regression model "mod" using the training set to predict the log visits. As a first step, we only use the laglogVisits variable (log visits from last month).
mod = lm(logVisits ~ laglogVisits, data = train)
summary(mod)
#* What's the out-of-sample R2 in the testing set for this simple model?
pred = predict(mod, newdata = test)
SSE = sum((test$logVisits - pred)^2)
SST = sum((test$logVisits - mean(test$logVisits))^2)
1-SSE/SST
# We see that the model achieves good predictive power already simply using the previous month's visits. To see if the other knowledge we have about the parks can improve the model, let's add these variables in a new model.
#The new model would have the following variables:
#
#laglogVisits, laglogVisitsYear, Year, Month, Region, ParkType, and cost
mod2 = lm(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train)
summary(mod2)
#In the new model, what's the out-of-sample R2 in the testing set?
pred2 = predict(mod2, newdata = test)
SSE2 = sum((test$logVisits-pred2)^2)
SST # remins the same i.e. base model remains the same.
1-SSE2/SST
# Regression Trees
library(rpart)
#In addition to the logistic regression model, we can also train a regression tree. Use the same set of variables as the previous problem (laglogVisits, laglogVisitsYear, Year, Month, Region, ParkType, and cost), train a regression tree with cp = 0.05.
caRt1 <- rpart(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train, method = "anova", cp = 0.05)
library(rpart.plot)
prp(caRt1) # How many different predicted values are there? 
summary(caRt1) 
#What is the out-of-sample R2 on the testing set?
predcaRt = predict(caRt1, newdata = test)
SSEcaRt = sum((test$logVisits - predcaRt)^2)
SST # remains the same
1-SSEcaRt/SST
#Regression Trees with CV
#▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
# refer to Recitation from analytic edge for further help
plot(100)
prp(caRt1)
# use cross-validation to imporve the caRt model / try different values of cp i.e. change the acceptable threshold for 'decrease the overall lack of fit by'
library(e1071)
?trainControl
library(caret)
?trainControl
?train
names(getModelInfo())
?train
ls()
0
ls()
library
library(ggplot2)
# use cross-validation to imporve the caRt model / try different values of cp i.e. change the acceptable threshold for 'decrease the overall lack of fit by'
library(e1071)
library(carel)
# use cross-validation to imporve the caRt model / try different values of cp i.e. change the acceptable threshold for 'decrease the overall lack of fit by'
library(e1071)
library(caret)
set.seed(201)
#Define cross validation experiment
numFolds = trainControl( method = "cv", number = 10 )
cpGrid = expand.grid( .cp = seq(0.0001, 0.005, 0.0001)) 
#Perform cross validation
train(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data=train, method-"rpart", trControl=numfolds, tuneGrid=cpGrid)
#Perform cross validation
train(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data=train, method="rpart", trControl=numfolds, tuneGrid=cpGrid)
#Perform cross validation
train(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data=train, method="rpart", trControl=numFolds, tuneGrid=cpGrid)
set.seed(201)
#Define cross validation experiment
numFolds = trainControl( method = "cv", number = 10 )
cpGrid = expand.grid( .cp = seq(0.0001, 0.0005, 0.00001)) 
#Perform cross validation
train(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data=train, method="rpart", trControl=numFolds, tuneGrid=cpGrid)
set.seed(201)
#Define cross validation experiment
numFolds = trainControl( method = "cv", number = 10 )
cpGrid = expand.grid( .cp = seq(0.0001, 0.005, 0.0001)) 
#Perform cross validation
train(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data=train, method="rpart", trControl=numFolds, tuneGrid=cpGrid)
#Perform cross validation
train(logVisits ~ . , data=train, method="rpart", trControl=numFolds, tuneGrid=cpGrid)
#Perform cross validation
train(logVisits ~ .,data=train, method="rpart", trControl=numFolds, tuneGrid=cpGrid)
#Rerun the regression tree on the training data, now using the cp value equal to the one selected in the previous problem (under the original range). Note: do not get the tree from the cross-validation directly.
caRt2 = rpart(lobVisits ~ . , data = train, method = "anova", cp = 0.0001)
#Rerun the regression tree on the training data, now using the cp value equal to the one selected in the previous problem (under the original range). Note: do not get the tree from the cross-validation directly.
caRt2 = rpart(logVisits ~ . , data = train, method = "anova", cp = 0.0001)
summary(caRt2)
names(caRt2)
caRt2
names(caRt2)
prp(caRt2)
#What is the out-of-sample R2 in the testing set?
predcaRt2 = predict(caRt2, newdata=test)
length(predcaRt2)
head(predcaRt2)
#What is the out-of-sample R2 in the testing set?
predcaRt2 = predict(caRt2, newdata=test)
SSEcaRt2 = sum((test$logVisits - predcaRt2)^2)
SST " remains the same
SSEcaRt2 = sum((test$logVisits - predcaRt2)^2)
SST " remains the same
SSEcaRt2 = sum((test$logVisits - predcaRt2)^2)
SST 
1-SSEcaRt2/SST
caRt2 <- rpart(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train, method = "anova", cp = 0.0001)
#What is the out-of-sample R2 in the testing set?
predcaRt2 = predict(caRt2, newdata=test)
SSEcaRt2 = sum((test$logVisits - predcaRt2)^2)
SST 
1-SSEcaRt2/SST
library(randomForrest)
library(randomForrest)
library(randomForest)
install.packages("randomForest")
1-SSEcaRt2/SST
summary(caRt2)
library(RandomForest)
library(RandomForest)
library(RandomForest)
set.seed(201)
library(randomForest)
rf = randomForest(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train)
sapply(train,class)
rf = randomForest(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train)
train2 = train
cols = c(2,3)
head(train2)
cols = c(2,3,6)
sapply(train2,class)
sapply(train2[cols],class)
train2[cols] <- as.factor(train2[cols])
head(train[cols])
class(train2[cols])
train2[cols] = lapply(train2[cols], factor)
sapply(train2, class)
test = randomForest(logVisits ~ laglogVisits + laglogVisitsYear + Year + Month + Region + ParkType + cost, data = train2)
test2 = test
test2[cols] = lapply(test2[cols], factor)
dim(test)
class(test2)
class(test)
test2=test
summary(test2)
summary(test)
#
visits$Month = as.factor(visits$Month)
#
train <- subset(visits, Year <=2014)
test <- subset(visits, Year > 2014)
#Let's build now a simple linear regression model "mod" using the training set to predict the log visits. As a first step, we only use the laglogVisits variable (log visits from last month).
dim(test)
dim(train)
test[cols] = lapply(test[cols], factor)
sapply(test, class)
predrf = predict(test2, newdata=test)
length(predrf)
head(predrf)
head(predcaRt2)
SSErf=sum((test$logVisits - predrf)^2)
1 - SSErf/SST
history
history(40)
history()
history()
history
history(1)
history(Inf)
?savehistory
getwd()
list.files()
system("ls -all")
system("ls")
getwd()
savehistory(file = "rf.Rhistory")
