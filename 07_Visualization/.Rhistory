stocksTrain = subset(stocks, spl == TRUE)
stocksTest = subset(stocks, spl == FALSE)
#   Then, use the stocksTrain data frame to train a logistic regression model (name it StocksModel) to predict PositiveDec using all the other variables as independent variables. Don't forget to add the argument family=binomial to your glm command.
stocksModel <- glm(PositiveDec ~ . , data = stocksTrain, family = "binomial")
#What is the overall accuracy on the training set, using a threshold of 0.5?
prLog_stocks = predict(stocksModel, type = "response") # Since the model was logit_reg I will not use ' type = "class" '
# Accuracy
table(stocksTrain$PositiveDec, prLog_stocks >= .5)
sum(diag(table(stocksTrain$PositiveDec, prLog_stocks >= .5)))/sum(table(stocksTrain$PositiveDec, prLog_stocks >= .5))
# Now obtain test set predictions from StocksModel. What is the overall accuracy of the model on the test, again using a threshold of 0.5?
prLog_stock_test <- predict(stocksModel, newdata = stocksTest, type = "response")
table(stocksTest$PositiveDec, prLog_stock_test >= .5)
sum(diag(table(stocksTest$PositiveDec, prLog_stock_test >= .5)))/sum(table(stocksTest$PositiveDec, prLog_stock_test >= .5))
# What is the accuracy on the test set of a baseline model that always predicts the most common outcome (PositiveDec = 1)?
table(stocksTrain$PositiveDec)
4427/(3679+4427)
#  Problem 3.1 - Clustering Stocks
# Now, let's cluster the stocks. The first step in this process is to remove the dependent variable using the following commands:
limitedTrain = stocksTrain
limitedTrain$PositiveDec = NULL
limitedTest = stocksTest
limitedTest$PositiveDec = NULL
# In the market segmentation assignment in this week's homework, you were introduced to the preProcess command from the caret package, which normalizes variables by subtracting by the mean and dividing by the standard deviation.
library(caret)
preproc = preProcess(limitedTrain)
normTrain = predict(preproc, limitedTrain)
normTest = predict(preproc, limitedTest)
# What is the mean of the ReturnJan variable in normTrain?
mean(normTrain$ReturnJan)
mean(normTest$ReturnJan)
# Set the random seed to 144 (it is important to do this again, even though we did it earlier). Run k-means clustering with 3 clusters on normTrain, storing the result in an object called km.
set.seed(144)
distance = dist(normTrain, method = "euclidean")
hclust
km = hclust(distance, method ="ward.D" )
km = kmeans(normTrain, centers = 3)
table(km)
table(km$cluster)
install.packages("flexclust")
library(flexclust)
km.kcca = as.kcca(km, normTrain)
clusterTrain = predict(km.kcca)
clusterTest = predict(km.kcca, newdata=normTest)
head(clusterTest)
table(clusterTest)
stockTrain1 = subset(stocksTrain, km$cluster == 1)
stocksTrain2 = subset(stocksTrain, km$cluster ==2)
stocksTrain3 = subset(stocksTrain, km$cluster = 3)
stocksTest1 = subset(stocksTest, km$cluster == 1)
stocksTest2 = subset(stocksTest, km$cluster == 2)
stocksTest3 = subset(stocksTest, km$cluster == 3)
stocksTrain1 = subset(stocksTrain, km$cluster == 1)
stocksTrain2 = subset(stocksTrain, km$cluster ==2)
stocksTrain3 = subset(stocksTrain, km$cluster = 3)
stocksTest1 = subset(stocksTest, km$cluster == 1)
stocksTest2 = subset(stocksTest, km$cluster == 2)
stocksTest3 = subset(stocksTest, km$cluster == 3)
stocksTrain3 = subset(stocksTrain, km$cluster == 3)
mean(stocksTrain1$PositiveDec)
mean(stocksTrain2$PositiveDec)
mean(stocksTrain3$PositiveDec)
StockModel1 <- glm(PositiveDec ~ ., data = stocksTrain1, family = "binomial")
StockModel2 <- glm(PositiveDec ~ ., data = stocksTrain2, family = "binomial")
StockModel3 <- glm(PositiveDec ~ ., data = stocksTrain3, family = "binomial")
summary(StockModel1)
names(summary(StockModel1))
summary(StockModel1)$coefficients
summary(StockModel1)$coefficients[1]
summary(StockModel1)$coefficients[,1]
rbind(summary(StockModel1)$coefficients[,1],
summary(StockModel2)$coefficeints[,1])
cbind(summary(StockModel1)$coefficients[,1],
summary(StockModel2)$coefficeints[,1])
cbind(summary(StockModel1)$coefficients[,1], summary(StockModel2)$coefficeints[,1])
cbind(summary(StockModel1)$coefficients[,1],
summary(StockModel2)$coefficeints[,1])
summary(StockModel1)$coefficients[,1]
summary(StockModel2)$coefficeints[,1])
summary(StockModel1)$coefficients[,1]
summary(StockModel2)$coefficeints[,1]
summary(StockModel2)$coefficeints[,1]
StockModel1 <- glm(PositiveDec ~ ., data = stocksTrain1, family = "binomial")
StockModel2 <- glm(PositiveDec ~ ., data = stocksTrain2, family = "binomial")
StockModel3 <- glm(PositiveDec ~ ., data = stocksTrain3, family = "binomial")
summary(StockModel2)
summary(StockModel1)$coefficients[,1]
summary(StockModel2)$coefficients[,1]
summary(StockModel3)$coefficients[,1]
rbind(
summary(StockModel1)$coefficients[,1],
summary(StockModel2)$coefficients[,1],
summary(StockModel3)$coefficients[,1]
)
cbind(
summary(StockModel1)$coefficients[,1],
summary(StockModel2)$coefficients[,1],
summary(StockModel3)$coefficients[,1]
)
cbind(
summary(StockModel1)$coefficients[,1],
summary(StockModel2)$coefficients[,1],
summary(StockModel3)$coefficients[,1]
)
predict
pr_stocks_md1 = predict(StockModel1, newdata = stocksTest1, type = "response")
pr_stocks_md2 = predict(StockModel2, newdata = stocksTest2, type = "response")
pr_stocks_md2 = predict(StockModel3, newdata = stocksTest3, type = "response")
pr_stocks_md2 = predict(StockModel2, newdata = stocksTest2, type = "response")
pr_stocks_md3 = predict(StockModel3, newdata = stocksTest3, type = "response")
table(stocksTest1$PositiveDec, pr_stocks_md1 >= .5)
sum(daig(table(stocksTest1$PositiveDec, pr_stocks_md1 >= .5)))/sum(table(stocksTest1$PositiveDec, pr_stocks_md1 >= .5))
sum(diag(table(stocksTest1$PositiveDec, pr_stocks_md1 >= .5)))/sum(table(stocksTest1$PositiveDec, pr_stocks_md1 >= .5))
sum(diag(table(stocksTest2$PositiveDec, pr_stocks_md2 >= .5)))/sum(table(stocksTest2$PositiveDec, pr_stocks_md2 >= .5))
sum(diag(table(stocksTest3$PositiveDec, pr_stocks_md3 >= .5)))/sum(table(stocksTest3$PositiveDec, pr_stocks_md3 >= .5))
length(clusterTrain)
length(km$cluster)
table(clusterTrain)
table(km$cluster)
PredictTest1 = predict(StockModel1, newdata = stocksTest1, type = "response")
PredictTest2 = predict(StockModel2, newdata = stocksTest2, type = "response")
PredictTest3 = predict(StockModel3, newdata = stocksTest3, type = "response")
sum(diag(table(stocksTest1$PositiveDec, pr_stocks_md1 >= .5)))/sum(table(stocksTest1$PositiveDec, pr_stocks_md1 > .5))
pr_stocks_md1 = predict(StockModel1, newdata = stocksTest1)
sum(diag(table(stocksTest1$PositiveDec, pr_stocks_md1 >= .5)))/sum(table(stocksTest1$PositiveDec, pr_stocks_md1 >= .5))
pr_stocks_md2 = predict(StockModel2, newdata = stocksTest2)
sum(diag(table(stocksTest2$PositiveDec, pr_stocks_md2 >= .5)))/sum(table(stocksTest2$PositiveDec, pr_stocks_md2 >= .5))
pr_stocks_md3 = predict(StockModel3, newdata = stocksTest3)
sum(diag(table(stocksTest3$PositiveDec, pr_stocks_md3 >= .5)))/sum(table(stocksTest3$PositiveDec, pr_stocks_md3 >= .5))
pr_stocks_md1 = predict(StockModel1, newdata = stocksTest1, type = "response")
pr_stocks_md2 = predict(StockModel2, newdata = stocksTest2, type = "response")
pr_stocks_md3 = predict(StockModel3, newdata = stocksTest3, type = "response")
sum(diag(table(stocksTest1$PositiveDec, pr_stocks_md1 > .5)))/sum(table(stocksTest1$PositiveDec, pr_stocks_md1 > .5))
sum(diag(table(stocksTest1$PositiveDec, pr_stocks_md1 > .5)))/sum(table(stocksTest1$PositiveDec, pr_stocks_md1 > .5))
sum(diag(table(stocksTest2$PositiveDec, pr_stocks_md2 > .5)))/sum(table(stocksTest2$PositiveDec, pr_stocks_md2 > .5))
sum(diag(table(stocksTest3$PositiveDec, pr_stocks_md3 > .5)))/sum(table(stocksTest3$PositiveDec, pr_stocks_md3 > .5))
stocksTrain1 = subset(stocksTrain, clusterTrain == 1)
stocksTrain2 = subset(stocksTrain, clusterTrain == 2)
stocksTrain3 = subset(stocksTrain, clusterTrain == 3)
stocksTest1 = subset(stocksTest, clusterTest == 1)
stocksTest2 = subset(stocksTest, clusterTest == 2)
stocksTest3 = subset(stocksTest, clusterTest == 3)
# Which training set data frame has the highest average value of the dependent variable?
mean(stocksTrain1$PositiveDec)
mean(stocksTrain2$PositiveDec)
mean(stocksTrain3$PositiveDec)
# Build logistic regression models StocksModel1, StocksModel2, and StocksModel3, which predict PositiveDec using all the other variables as independent variables. StocksModel1 should be trained on stocksTrain1, StocksModel2 should be trained on stocksTrain2, and StocksModel3 should be trained on stocksTrain3.
StockModel1 <- glm(PositiveDec ~ ., data = stocksTrain1, family = "binomial")
StockModel2 <- glm(PositiveDec ~ ., data = stocksTrain2, family = "binomial")
StockModel3 <- glm(PositiveDec ~ ., data = stocksTrain3, family = "binomial")
# Which variables have a positive sign for the coefficient in at least one of StocksModel1, StocksModel2, and StocksModel3 and a negative sign for the coefficient in at least one of StocksModel1, StocksModel2, and StocksModel3? Select all that apply.
cbind(
summary(StockModel1)$coefficients[,1],
summary(StockModel2)$coefficients[,1],
summary(StockModel3)$coefficients[,1]
)
# Using StocksModel1, make test-set predictions called PredictTest1 on the data frame stocksTest1. Using StocksModel2, make test-set predictions called PredictTest2 on the data frame stocksTest2. Using StocksModel3, make test-set predictions called PredictTest3 on the data frame stocksTest3.
pr_stocks_md1 = predict(StockModel1, newdata = stocksTest1, type = "response")
pr_stocks_md2 = predict(StockModel2, newdata = stocksTest2, type = "response")
pr_stocks_md3 = predict(StockModel3, newdata = stocksTest3, type = "response")
# Accuracy
sum(diag(table(stocksTest1$PositiveDec, pr_stocks_md1 > .5)))/sum(table(stocksTest1$PositiveDec, pr_stocks_md1 > .5))
sum(diag(table(stocksTest2$PositiveDec, pr_stocks_md2 > .5)))/sum(table(stocksTest2$PositiveDec, pr_stocks_md2 > .5))
sum(diag(table(stocksTest3$PositiveDec, pr_stocks_md3 > .5)))/sum(table(stocksTest3$PositiveDec, pr_stocks_md3 > .5))
Allpredictions = c(pr_stocks_md1, pr_stocks_md2, pr_stocks_md3)
AllOutcomes = c(stocksTest1$PositiveDec, stocksTest2$PositiveDec, stocksTest3$PositiveDec)
table(AllOutcomes, Allpredictions >= .5)
sum(diag(table(AllOutcomes, Allpredictions >= .5)))/sum(table(AllOutcomes, Allpredictions >= .5))
setwd("c:/Study/")
rm(list=ls())
gc()
exit
exit()
quit()
install.packages("data.world")
library(data.world)
data.world::set_config(save_config(auth_token = "eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50OmJsc2luZ2giLCJpc3MiOiJhZ2VudDpibHNpbmdoOjpiMDBhYTA4Ny0yNjM3LTQyYjUtODEyZS0wMGJkODJkNWI4MDEiLCJpYXQiOjE1MDExMTc2NjAsInJvbGUiOlsidXNlcl9hcGlfd3JpdGUiLCJ1c2VyX2FwaV9yZWFkIl0sImdlbmVyYWwtcHVycG9zZSI6dHJ1ZX0.EXd3wA_UdOCUJxZ18hrF1kuu2FtSZsjMqjcupC0Td-T7Q86pOHGuCOxppnvsb7NaNuWnrK2A5fr1UcHduw_9rQ"))
vignette("quickstart", package = "data.world")
??data.world
intro_ds <- "https://data.world/blsingh/sba-loan-qualification-criteria"
team_df <- data.world::query(
data.world::qry_sql("show tables"),
dataset = intro_ds
)
team_df
?qry_sql
intro_ds
intro_def <- dwapi::get_dataset(intro_ds)
intro_def$description
?dwapi
list_tables(intro_ds)
intro_ds <- "https://data.world/nerb/sba-loan-guarantee-data"
list_tables(intro_ds)
intro_def$description
ls()
class(intro_def)
str(intro_def)
intro_def <- dwapi::get_dataset(intro_ds)
intro_def$description
ls()
str(intro_def)
ls()
object.size("intro_def")
list_table(intro_ds)
list_tables(intro_ds)
qry_sql
?qry_sql
getwd()
exit()
quit()
ls()
plot(100)
plot(100,100)
plot(rnorm(100))
q()
getwd()
setwd("./Analy_Edge/07_Visualization/")
list.files()
mvt = read.csv("mvt.csv", stringsAsFactors=FALSE)
str(mvt)
mvt$Date = strptime(mvt$Date, format="%m/%d/%y %H:%M")
mvt$Weekday = weekdays(mvt$Date)
mvt$Hour = mvt$Date$hour
str(mvt)
table(mvt$Weekday)
WeekdayCounts = as.data.frame(table(mvt$Weekday))
str(WeekdayCounts)
library(ggplot2)
ggplot(WeekdayCounts, aes(x=Var1, y=Freq)) + geom_line(aes(group=1))
WeekdayCounts$Var1 = factor(WeekdayCounts$Var1, ordered=TRUE, levels=c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday","Saturday"))
ggplot(WeekdayCounts, aes(x=Var1, y=Freq)) + geom_line(aes(group=1))
ggplot(WeekdayCounts, aes(x=Var1, y=Freq)) + geom_line(aes(group=1)) + xlab("Day of the Week") + ylab("Total Motor Vehicle Thefts")
table(mvt$Weekday, mvt$Hour)
DayHourCounts = as.data.frame(table(mvt$Weekday, mvt$Hour))
str(DayHourCounts)
DayHourCounts$Hour = as.numeric(as.character(DayHourCounts$Var2))
ggplot(DayHourCounts, aes(x=Hour, y=Freq)) + geom_line(aes(group=Var1))
str(DayHourCounts)
ggplot(DayHourCounts, aes(x=Hour, y=Freq)) + geom_line(aes(group=Var1, color=Var1), size=2)
DayHourCounts$Type = ifelse((DayHourCounts$Var1 == "Sunday") | (DayHourCounts$Var1 == "Saturday"), "Weekend", "Weekday")
ggplot(DayHourCounts, aes(x=Hour, y=Freq)) + geom_line(aes(group=Var1, color=Type), size=2)
ggplot(DayHourCounts, aes(x=Hour, y=Freq)) + geom_line(aes(group=Var1, color=Type), size=2, alpha=0.5)
DayHourCounts$Var1 = factor(DayHourCounts$Var1, ordered=TRUE, levels=c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
ggplot(DayHourCounts, aes(x = Hour, y = Var1)) + geom_tile(aes(fill = Freq))
ggplot(DayHourCounts, aes(x = Hour, y = Var1)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(name="Total MV Thefts") + theme(axis.title.y = element_blank())
ggplot(DayHourCounts, aes(x = Hour, y = Var1)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(name="Total MV Thefts", low="white", high="red") + theme(axis.title.y = element_blank())
ggplot(DayHourCounts, aes(x = Hour, y = Var1)) + geom_tile(aes(fill = Freq)) + scale_fill_gradient(name="Total MV Thefts", low="white", high="black") + theme(axis.title.y = element_blank())
# VIDEO 5 - Maps
# VIDEO 5 - Maps
install.packages("maps")
install.packages("ggmap")
library(maps)
library(ggmap)
chicago = get_map(location = "chicago", zoom = 11)
ggmap(chicago)
ggmap(chicago) + geom_point(data = mvt[1:100,], aes(x = Longitude, y = Latitude))
LatLonCounts = as.data.frame(table(round(mvt$Longitude,2), round(mvt$Latitude,2)))
str(LatLonCounts)
LatLonCounts$Long = as.numeric(as.character(LatLonCounts$Var1))
LatLonCounts$Lat = as.numeric(as.character(LatLonCounts$Var2))
ggmap(chicago) + geom_point(data = LatLonCounts, aes(x = Long, y = Lat, color = Freq, size=Freq))
ggmap(chicago) + geom_point(data = mvt[1:100,], aes(x = Longitude, y = Latitude))
ggmap(chicago) + geom_point(data = LatLonCounts, aes(x = Long, y = Lat, color = Freq, size=Freq))
ggmap(chicago) + geom_point(data = LatLonCounts, aes(x = Long, y = Lat, color = Freq, size=Freq)) + scale_colour_gradient(low="yellow", high="red")
ggmap(chicago) + geom_tile(data = LatLonCounts, aes(x = Long, y = Lat, alpha = Freq), fill="red")
str(LatLonCounts)
LatLonCounts2 <- subset(LatLonCounts, Freq != 0)
dim(LatLongCounts2)
dim(LatLonCounts2)
dim(LatLonCounts)
1638-686
murders = read.csv("murders.csv")
str(murders)
statesMap = map_data("state")
str(statesMap)
ggplot(statesMap, aes(x = long, y = lat, group = group)) + geom_polygon(fill = "white", color = "black")
str(murders)
murders$region = tolower(murders$State)
str(murders)
a=murders$State
class(a)
class(tolower(a))
rm(a)
?map_data
get_map
map_data
murderMap = merge(statesMap, murders, by="region")
str(murderMap)
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = Murders)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")
str(stateMap)
str(statesMap)
str(murders)
a = table(statesMap$long, statesMap$lat)
dim(a)
unique(a)
head(a)
str(murderMap)
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = GunOwnership)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend", limits = c(0,10))
ggplot(murderMap, aes(x = long, y = lat, group = group, fill = GunOwnership)) + geom_polygon(color = "black") + scale_fill_gradient(low = "black", high = "red", guide = "legend")
table(murderMap$GunOwnership)
str(murders)
unique(murders$GunOwnership)
lenght(unique(murders$GunOwnership))
length(unique(murders$GunOwnership))
talbe(murders$State, murders$GunOwnership)
table(murders$State, murders$GunOwnership)
table(murders$GunOwnership, murders$State)
table(murders$State, murders$GunOwnership)
table(murders$State, murders$GunOwnership)
names(table(murders$State, murders$GunOwnership))
as.data.frame((table(murders$State, murders$GunOwnership)))
a = as.data.frame((table(murders$State, murders$GunOwnership)))
library(dplry)
library(dplyr)
names(a)
head(a)
head(sort(a$Var2, decreasing = T))
a %>% filter(Var2 == 0.597, Freq ==1)
a %>% filter(Freq==1)
a %>% filter(Freq==1) %>% arrange(sort(Var2))
library(ggplot2)
library(maps)
library(ggmap)
statesMap = map_data("states")
library(maps)
statesMap = map_data("states")
statesMap = map_data("state")
unique(statesMap$group)
str(statesMap)
ggplot(statesMap, aes(x = long, y = lat, group = group)) + geom_polygon(fill = "white", color = "black")
polling = read.csv("PollingImputed.csv")
head(polling)
Train = subset(polling, Year <2012)
unique(polling$Year)
Test = subset(polling, Year == 2012)
mod2 = glm(Republican~SurveyUSA+DiffCount, data=Train, family="binomial")
TestPrediction = predict(mod2, newdata=Test, type="response")
TestPredictionBinary = as.numeric(TestPrediction > 0.5)
table(TestPredictionBinary)
table(TestPrediction)
predictionDataFrame = data.frame(TestPrediction, TestPredictionBinary, Test$State)
table(TestPredictionBinary)
mean(TestPrediction)
predictionDataFrame$region = tolower(predictionDataFrame$Test.State)
predictionMap = merge(statesMap, predictionDataFrame, by = "region")
dim(predictionMap)
head(predictionMap)
ls()
head(statesMap)
summary(statesMap$order)
predictionMap = predictionMap[order(predictionMap$order),]# What happens is the when we merge the order was messed up
dim(predictionMap)
dim(statesMap)
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary)) + geom_polygon(color = "black")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary))+ geom_polygon(color = "black") + scale_fill_gradient(low = "blue", high = "red", guide = "legend", breaks= c(0,1), labels = c("Democrat", "Republican"), name = "Prediction 2012")
head(predictionMap)
predictionMap %>% filter(region == 'florida')
predictionMap %>% filter(region == 'florida') %>% select(TestPrediction)
?geom_polygon
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary))+ geom_polygon(color = "black", linetype = 2) + scale_fill_gradient(low = "blue", high = "red", guide = "legend", breaks= c(0,1), labels = c("Democrat", "Republican"), name = "Prediction 2012")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary))+ geom_polygon(color = "black", linetype = 2, size =2) + scale_fill_gradient(low = "blue", high = "red", guide = "legend", breaks= c(0,1), labels = c("Democrat", "Republican"), name = "Prediction 2012")
ggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary))+ geom_polygon(color = "black", alpha =.3) + scale_fill_gradient(low = "blue", high = "red", guide = "legend", breaks= c(0,1), labels = c("Democrat", "Republican"), name = "Prediction 2012")
edges = read.csv("edges.csv")
users = read.csv("users.csv")
str(users)
length(unique(users$id))
str(edges)
length(unique(edges$V1))
length(unique(edges$V2))
library(dplyr)
merges
merge
?merge
c = intersect(edges$V1, users$id)
dim(c)
length(c)
d = intersect(edges$V2, users$id)
lenght(d)
length(d)
intersect(c,d)
str(edges)
e %>% edges %>% group_by(V1) %>% summary(count=n())
edges %>% group_by(V1) %>% summary(count=n())
edges %>% group_by(V1) %>% summarise(count=n())
e=edges %>% group_by(V1) %>% summarise(count=n())
names(e)
mean(e$count)
dim(edges)
unique(edges)
length(unique(edges$V1))
length(unique(c(edges$V1, edges$V2)))
length(unique(users$id))
head(users)
intersect
outersect
outersect
x = letters[1:3]
#[1] "a" "b" "c"
y = letters[2:4]
#[1] "b" "c" "d"
intersect(a,b)
intersect(x,y)
unique(c(x,y))
length(unique(edges$V1,edges$V2))
dim(edges)
a = merge(edges, users, intersect(edges$V1,users$id))
a = merge(edges, users, by=
intersect(edges$V1,users$id))
a = merge(edges, users, by=intersect(edges$V1,users$id))
a = merge(edges, users, by.x=edges$V1,by.y=users$id)
146*2
table(users$school)
str(users)
table(users$school,useNA = T)
?table
table(users$school,useNA = "always")
users %>% filter(school %in% c("A","AB"))
head(users)
a = users %>% filter(school %in% c("A","AB"))
table(a$locale)
table(users$locale,users$school)
table(users$gender, users$school)
head(users)
table(users$gender, users$school)
"wer"
" Problem 2.1 - Creating a Network
1 point possible (graded)
We will be using the igraph package to visualize networks; install and load this package using the install.packages and library commands.
We can create a new graph object using the graph.data.frame() function. Based on ?graph.data.frame, which of the following commands will create a graph g describing our social network, with the attributes of each user correctly loaded?
Note: A directed graph is one where the edges only go one way -- they point from one vertex to another. The other option is an undirected graph, which means that the relations between the vertices are symmetric. "
install.packages(igraph)
install.packages("igraph")
library(igraph)
?graph.data.frame
g = graph.data.frame(edges, FALSE, users)
plot(g, vertex.size=5, vertex.label=NA)
degree(g)
class(degree(g))
degree(g)[(degree(g))>12]
degree(g)[(degree(g))>=10]
length(degree(g)[(degree(g))>=10])
V(g)
V(g)$size
degree(g)/2
V(g)$size = degree(g)/2+2
degree(g)/2+2
sort(degree(g)/2+2)
plot(g, vertex.label=NA)
We can update the colors by setting the color to black for all vertices, than setting it to red for the vertices with gender A and setting it to gray for the vertices with gender B:"
V(g)$color = "black"
V(g)$color[V(g)$gender == "A"] = "red"
V(g)$color[V(g)$gender == "B"] = "gray"
plot(g)
plot(g, vertex.label = NA)
color()
colours()
V(g)$color[V(g)$school == "A"] = "yellow"
V(g)$color[V(g)$school == "AB"] = "pink"
plot(g, vertex.label = NA)
?graph.data.frame
V(g)$color[V(g)$school == "AB"] = "green"
plot(g, vertex.label = NA)
graph.data.frame
summary(users$locale)
V(g)$color = "black"
V(g)$color[V(g)$locale == "A"] = "red"
V(g)$color[V(g)$locale == "B"] = "gray"
plot(g, vertex.label = NA)
V(g)$color[V(g)$locale == "B"] = "yellow"
plot(g, vertex.label = NA)
?igraph.plotting
rglplot(g)
install.packages("rgl")
library(rgl)
rglplot(g, vertex.label = NA)
tweets = read.csv("tweets.csv")
tweets = read.csv("tweets.csv", stringsAsFactors = F)
tweets = read.csv("tweets.csv", stringsAsFactors = FALSE)
str(tweets)
names(tweets)
corpus = corpus(tweets$Tweet)
library(tm)
corpus = corpus(tweets$Tweet)
corpus = Vcorpus(VectorSource(tweets$Tweet))
Vcorpus
corpus = VCorpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus ,content_transformer(tolower))
corpus = tm_map(corpus , removePunctuation)
corpus = tm_map(corpus , removeWords, stopwords("english"))
dtm = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(dtm))
dim(allTweets)
tbl_df(allTweets)
library(wordcloud)
?wordcloud
wordcloud(colnames(allTweets), colSums(allTweets), scale = c(2,0.25))
corpus = VCorpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus ,content_transformer(tolower))
corpus = tm_map(corpus , removePunctuation)
corpus = tm_map(corpus , removeWords, c("apple",stopwords("english")))
dtm = DocumentTermMatrix(corpus)
allTweets = as.data.frame(as.matrix(dtm))
dim(allTweets)
library(wordcloud)
?wordcloud
wordcloud(colnames(allTweets), colSums(allTweets), scale = c(2,0.25))
wordcloud(colnames(allTweets), colSums(allTweets),random.order = F, scale = c(2,0.25))
brewer.pal("Accent")
display.brewer.all()
wordcloud(colnames(allTweets), colSums(allTweets),random.order = F, scale = c(2,0.25), colors=brewer.pal(9, "Blues"))
data(mtcars)
head(mtcars)
q()
